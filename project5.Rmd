---
title: "Nearest Neighbour Method for Head Cell Decoding"
author: "Rasmus Munter"
date: "May 11, 2018"
output:
    pdf_document:
        fig_caption: true
        fig_height: 3
header-includes:
    - \usepackage{float}
---

```{r include=FALSE}
knitr::opts_chunk$set(include=FALSE, echo=FALSE, cache=TRUE, eval.after = "fig.cap", out.extra = '', fig.pos = 'H')

library(ggplot2)
library(ggpubr)
library(dplyr)
library(knitr)
library(directlabels)
library(reshape)

source('./util/problem5.R')

theme_set(theme_bw())

# For knitting only
knit_child('project2.Rmd', options=list(include=FALSE))
```

Head direction(HD) cells in mice are a neuron population that have a clear relationship with the compass head direction of mice. This paper decodes HD cell response using the nearest neighbor method. The model performance was generally accurate but not precise with a 20.29 RMSE and an correct angle bin prediction accuracy of 21%. A possible route for improvement is to use an average of the nearest neighbors to increase precision. Further analysis showed the choice of smoothing window had a relatively large effect on performance. When using mutual information to assess smoothing window size the HD cells showed signs of temporal coding. However it is suggested that this was simply an artifact caused by mouse movement across the large smoothing window used and that it is primarily rate coding HD cells rely on.

## Introduction

Neural decoding is the process of trying to calculate the stimulus that caused a certain measured response. Performing decoding can give insight into how the brain encodes information. The complexity of this issues varies widely with both the method used to decode and the response that is being decoded. This paper addresses a simple variation of this problem by attempting to decode head direction(HD) cells in mice. HD cells are neurons where the firing rate is directly related to the compass heading of the mouse, with the firing rate forming a clear peak at a certain angle, while remaining low otherwise$^1$. Since this is a simple encoding of head direction the reverse process, decoding, should be simple as well. This paper applies and analyses one of the simpler decoding methods, the nearest neighbor algorithm$^2$, on a population of neurons in areas of the mouse brain known to contain HD cells. It then proceeds to use this model to discuss the type of encoding head cells might primarily use.

## Methods

The cell population recordings were taken from an experiment by Adrien Peyrache$^3$ and consists of measurements from the thalamus and post subiculum. Due to time and computing power constraints the data was constrained to recordings from mouse 28 in session 14038 consisting of 80 cells. The continuous head angle data, which can be viewed as the stimulus, was split into 40 non-overlapping 9 degree bins. The session was then was split into two unshuffled halves, with the first half as the training set and the other half as the test set.

Reference vectors for each of the 40 angle bins was calculated. This was done by calculating the average firing rate in each bin and dividing by the occupancy using only data from the training set. The population vector was then calculated on the test set by calculating the average firing rate based on a sliding 100 ms smoothing window. Finally population vectors for timestamps that had no angle data were removed before performing predictions.

The predictions were calculated using a variant of the nearest neighbor algorithm. The pairwise Pearson correlation was calculated between each population vector and the 40 reference vectors. The prediction was then set to be the angle corresponding to the the reference vector with the highest correlation with the population vector.

Part of the analysis of the results uses mutual information between single HD neurons response and the stimulus. The HD cells were found by plotting the tuning curves for all 80 cells that had a minimum firing rate under 10. Then tuning curves with distinct single peaks with a low firing rate otherwise were classified as head direction cells.

Mutual information is a measure of the reduction in uncertainty about a variable given knowledge about another variable$^2$. Denoting the neuron response as $R$ and the stimulus as $S$ the equation for mutual information is 

$$
I(S;R) = \sum_{s,r} P(r)P(s|r)log_2\frac{P(s|r)}{P(s)}.
$$

This however requires knowledge about the distributions of the response, stimulus and stimulus conditioned on the response. The simplest approximation is to use the normalized frequency of the responses and stimulus from the experimental data. This is known as the plug-in method$^4$.

## Results

```{r}
#Split the dataset
data_split = train_test_split(data3)
training_data = data_split$training_data
test_data = data_split$test_data

# From earlier runs, look at rough draft to see how this was run
load('predictions.RData')
load('momentaryVectors.RData')
smoothing_windows = as.numeric(names(momentaryVectors))
# Data from smoothing_window 100
indexs = momentaryVectors[['100']]$usable_indexs
prediction = predictions[[which(smoothing_windows==100)]]
```

```{r "Target and prediction dataframe for plotting"}

#Create target
bin_size = 9
target = test_data$awake_angle[indexs]*(180/pi) # Convert awake indexs to degrees

#Bin target
target_bin_index = ceiling(target/bin_size)
target_binned = (target_bin_index-1)*bin_size + bin_size/2
target_binned_df = data.frame(time=1:length(target), angle=target_binned, type='target')

#Predictions
prediction_df = data.frame(time=1:length(prediction), angle=prediction, type='prediction')

df_binned = rbind(prediction_df, target_binned_df)
```


### Model perfomance

Running the described method on the test set resulted in an root mean square error(RMSE) of 20.29 and a correct prediction accuracy at 21%.

```{r "prediction metrics"}
rmse=sum(sqrt(angleDiff(prediction,target)^2))/length(prediction)

confusion_mat = table(prediction=prediction_df$angle, target=target_binned)
confusion_mat = as.data.frame(sweep(confusion_mat, 1, rowSums(confusion_mat), '/')) # Normalize by row

accuracy = sum(angleDiff(prediction,target_binned )<1)/length(prediction)
accuracyslack = sum(angleDiff(prediction,target_binned )<10)/length(prediction)
```

In figure \ref{confMat}a the distribution shows that this is in general due to a lot of predictions being a few bins off the target value rather than a few extreme errors. 51.6% of the predictions were correct or one bin off the target. However, the confusion matrix in \ref{confMat}b does show that certain angles caused the model to frequently predict a far off angle.

```{r include=TRUE, fig.cap=caption}
p1 = ggplot(data=data.frame(error=angleDiff(target,prediction))) + 
    geom_histogram(aes(x=error, y=..density..), color='white', binwidth=9)

# Due to table() angles are factors. Thus taking as.double converts them to the bin index
p2 = ggplot(data=confusion_mat) + 
    geom_raster(aes(x=as.double(prediction), y=as.double(target), fill=Freq)) +
    scale_fill_distiller(palette='Spectral') +
    theme(legend.position="bottom") +
    coord_fixed() + 
    labs(x="Predicted Bin", y="True bin")


ggarrange(p1,p2,nrow=1,ncol=2, labels="AUTO")
caption = "\\label{confMat}\\textbf{Prediction error overview:} A: The distribution of prediction error binned into the 40 angle bins. B: Confusion matrix"
```

```{r include=TRUE, fig.cap="\\label{decoding}\\textbf{Visualization of model performance on test set:} Two 1000 ms long time  periods with the true head direction labeled as target and the corresponding model prediction"}

df_sample_plot = df_binned %>% mutate(timeperiod = ceiling(time/1000)) %>% filter(timeperiod==2 | timeperiod == 39)
# Selected out time period 2 and 39 to be good samples of the decoder working/failing
df_sample_plot[df_sample_plot$timeperiod==2,]$timeperiod='A'
df_sample_plot[df_sample_plot$timeperiod==39,]$timeperiod='B'


ggplot(data=df_sample_plot) +
    geom_point(aes(x=time, y=angle, color=type), fill='white', shape=21, alpha=1) + 
    facet_wrap(~timeperiod, scale="free_x") +
    scale_color_brewer(type='qual', palette=7) +
    coord_cartesian(ylim=c(0,360)) + 
    labs(x='Time(ms)', y='Angle(degrees)')

```


Figure \ref{decoding} shows samples of the decoders output versus the true head angle. Here the same issues appear, with most predictions being one to two bins off the correct angle. It's worth noting that in Figure \ref{decoding}A the prediction value varies up to 7 bins despite the target value staying constant. Figure \ref{decoding}B also shows results from a target angle from where the confusion matrix showed large errors in prediction. Here large jumps between the correct angle and a far off angle are observed. Once again this changes in the prediction occur despite no change in the target angle.

### Smoothing window size

The choice of smoothing window effects the model as it removes the temporal information of when the timespikes occurred. The reason smoothing windows are used are to capture firing rate encoding and to decrease random noise by averaging over a longer time period. However, choosing larger smoothing window sizes leads to a loss of the temporal relation between the spikes that were recorded$^2$. Figure \ref{RMSE} shows the change in the RMSE and the mutual information for different choices of bin size. In figure \ref{RMSE}A, as the smoothing window increases, the RMSE decreases up until a bin size of ~300 ms, before it starts increasing again. Figure \ref{RMSE}B shows the mutual information generally decreases with bin size. However it is also important to keep in mind the scale of the information rate. The decrease in information rate is relatively small, especially after passing 300 ms bins where some HD cells mutual information flatten out. This could explain why the RMSE decreases despite the mutual information rate decreasing as well.

```{r include=TRUE, fig.cap=caption}
rmse = rep(0,length(smoothing_windows))


# Good cells
# T8C6, T8C7, T9C11, T9C12, T9C13, T9C14, T5C5, T5C3, T9C10 
cell_names = c('T8C6', 'T8C7', 'T9C11', 'T9C12', 'T9C13', 'T9C14', 'T5C5', 'T5C3', 'T9C10')
cell_indexs = which(data3$cellnames %in% cell_names)

info = matrix(0, ncol=length(cell_indexs), nrow=length(smoothing_windows))
for(i in 1:length(smoothing_windows)){
    
    target = test_data$awake_angle[momentaryVectors[[i]]$usable_indexs]*(180/pi)
    target_bin_index = ceiling(target/bin_size)
    target_binned = (target_bin_index-1)*bin_size + bin_size/2
    
    rmse[i]=sum(sqrt(angleDiff(predictions[[i]],target)^2))/length(prediction)
    
    
    for(j in 1:length(cell_indexs)){
        # Divide by seconds for rate
        info[i,j] = calculateMutualInformation(target_binned, momentaryVectors[[i]]$momentary_population_matrix, j, smoothing_windows[i])/((2*smoothing_windows[i])/1000)
    }
}

# DF for plotting
colnames(info)=cell_names
info = melt(info)
info$smoothing_windows = smoothing_windows[info$X1]

p1=ggplot(data=data.frame(window=smoothing_windows, rmse=rmse)) +
    geom_line(aes(x=window, y=rmse)) +
    geom_point(aes(x=window, y=rmse), shape=21, fill='white') +
    labs(y='Root Mean Square Error', x="Smoothing window size(ms)")
p2=ggplot(data=info) +
    geom_line(aes(x=smoothing_windows, y=value, color=X2)) + 
    geom_point(aes(x=smoothing_windows, y=value, color=X2), shape=21, fill='white')+
    labs(y='Information rate(bits/s)', x="Smoothing window size(ms)", color='Cell')

ggarrange(p1,p2, nrow=1,ncol=2, labels='AUTO')
caption="\\label{RMSE} \\textbf{Effect of smoothing window size:} A: Root mean square error and B: Informaton rate for HD cells plotted against the smoothing window used. Cellnames are denoted by the tetrode number and then the corresponding cell number"
```


## Discussion

The performance of the nearest neighbor method on HD cell decoding was in general accurate but not precise. In addition it was shown that the models predictions were unstable and at certain angles the method could predict values almost 180 degrees apart despite the head angle staying the same. This is to be expected from the nearest neighbor algorithm as it is a method with high variance meaning small changes in the model input can lead to large changes in the output$^5$. One simple modification that could lower the variance of the model would be use what is known as the K nearest neighbor algorithm. This algorithm bases its prediction on the average of a set of nearest neighbors instead of only the closest one.

It is important to note that using K nearest neighbors would most likely not fix the large mispredictions at certain angles. It is harder to pinpoint exactly why the model fails at certain angles as there are multiple reasons this could be happening. One educated guess is that too many of the neurons in the population are not head direction cells but that coincidentally correlate well at certain angles. Picking out the head cells and creating the model purely on these might yield better results.

The choice of smoothing window was shown to have an effect on the models performance. The 300 ms smoothing window had the lowest RMSE on the test set. However, the information rate steadily decreased for most neurons upon increasing the smoothing window. This suggests that there is some temporal encoding in the head direction cell$^2$. Intuitively this makes little sense as there is a clear relation between firing rate and head angle$^1$ and so what additional benefit temporal coding would have is unclear. It also seems unlikely since the RMSE is much lower for large window sizes. I searched to find relevant literature on the topic but found no results where the information rate had been used to investigate the head cell encoding.  

The most likely cause of this issue is the movement speed of mice. With a smoothing window of 100 ms one is calculating the average firing rate for 200 ms. In efforts to find angular head velocity cells, mice have been recorded changing head direction with a velocity over $270$ degrees per second$^6$. This means a mouse could possibly change its head direction by 6 angle bins over the 200 ms time period. This means as the smoothing window is increased the relation between the firing rate and angle is weakened which would explain a lower calculated information rate. Attempting to ensure that the mouse remains in the same angle bin during one smoothing window would then require a 15 ms time period assuming the mouse started with it's head direction in the middle of the bin. This corresponds to a 7.5 ms smoothing window which matches the relatively large increase in mutual information from a smoothing window of 5 ms to 50 ms.

Looking past this issue, another possible problem with the mutual information rate is the sampling bias. Sampling bias is a known issue when using the plug in principle to estimate the mutual information as there is an underlying assumption that the data samples recorded represent the true distribution. There exist multiple bias correction methods that could be used to check the effect this has on the information rate relation to smoothing window size$^4$.

Finally it is important to note that despite the decrease in information rate with smoothing window size the best model by the RMSE metric used a large smoothing window of 300 ms. This indicates that the noise reduction of averaging the firing rate over a larger time period played a larger role than the issues that come with the larger smoothing window. Based on the results from this analysis it seems likely that the head direction cells depend more on rate encoding than temporal encoding.

## References

1. Taube JS, Muller RU, Ranck JB Jr. (1990) Head-direction cells recorded from the postsubiculum in freely moving rats. I. Description and quantitative analysis. J Neurosci. 10(2):420-35.

2. Quiroga RQ, Panzeri S (2013) Principles of Neural Coding. In: Decoding and Information Theory in Neuroscience. Quiroga RQ, Panzeri S. CRC Press, Boca Raton, USA. 

3. Peyrache A, Lacroix MM, Petersen PC et al. (2015) Internally organized mechanisms of the head direction sense. Nat Neurosci 18(4):560-575.

4. Panzeri S, Senatore R, Montemurro MA et al. (2007) Correcting for the sampling bias problem in spike train information measures. J Neurophysiol. 98(3):1064-72

5. Hastie T, Tibshirani R, Friedman J. (2009) The Elements of Statistical Learning. In: Overview of Supervised Learning. Hastie T, Tibshirani R, Friedman J. Springer, New York, USA.

6. Stackman RW, Taube JS. (1998) Firing Properties of Rat Lateral Mammillary Single Units: Head Direction, Head Pitch, and Angular Head Velocity. J Neurosci. 15;23(4):1555-1556



```{r}
ggplot(data=tuning_curves3, aes(x=angle_bins, y=firing_rate, color=cell)) +
    facet_wrap( ~ tetrode, ncol=4, scale='free_y') +
    geom_line() +
    geom_dl(aes(label=cell), color='black', method=list("top.bumpup", cex=0.8))
ggplot(data=tuning_curves3, aes(x=angle_bins, y=firing_rate, color=tetrode)) +
    facet_wrap( ~ cellname, ncol=4, scale='free_y') +
    geom_line() +
    theme(strip.text.x = element_text(size = 6),
          axis.ticks.x = element_blank(), axis.ticks.y = element_blank(),
          axis.text.x = element_blank(), axis.text.y = element_blank())
```
